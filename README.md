# BistabilityExample

In my sketch I was trying to present a bistable image / animation which draws from three sources:

—1. Ambiguity of interpretation of 3D objects looked at from a certain angle (in this case just a 2D projection of a 3D object on the computers screen) similar to Necker cube (Louis Albert Necker 1786–1861). 

In the above kind of “illusion” our brain is trying the figure out if the theoretical frontal polygon of the object is really frontal / directed towards them, or maybe it’s not and the viewer is actually looking inside the object, where the same perspective of lines could be interpreted as looking at the inner surface of the back wall of the object. 

—2. Face like shape: At the beginning of my sketch what is actually concave view into the object is likely to be interpreted as something convex and resembling animal face (perhaps of a bird, or a snake… ). I think this face-like shape strengthens the bistability effect. 

As follows from strategies observed in many researched bistable images we are programmed to recognise faces in most of shapes that might slightly suggest them and this form of perception might be more primal, arrive faster in the brain and override the tendency to interpret the mesh element in context of being concave / convex.

To contextualise the combination of 3D ambiguity and effect of a face that I have chosen to present I could also bring out demonstration on the link: https://www.youtube.com/watch?v=ORoTCBrCKIQ (although it is best to watch it with no sound as it is a loud TV show).

—3. Effect of bistability of 3D graphics when mesh is cut through the screen surface at z=0. 

I used an organic-shaped kind of 3D mesh and cut the projection through at z=0 as discussed above. 

Usually when we rotate such objects it leads to ambitious / bistable interpretations respective to the choice of the viewer. It is a question of a “decision” of the brain if what it interprets is a frontal peak or wall or a concave part inside an object. 

Since the only indicator are the lines of the mesh and the shading which are sketchy and as such ambiguous we can perceive image in two ways. 

I think such model could be also reproduced in a “real world” space, although in such a case we would have to face realistic lights and surfaces.

It takes some effort to imagine that initially looked at “face” is a concave element, however after pressing any key my mesh starts rotating around Y axis. Then the brain should have the chance to reinterpret the image. 

When rotating the image can became non-ambiguous as we realise that the “face” was a concave element and the projection of the object is just cut through screen. After pressing any key again we come back to the beginning, and we can see if after seeing the object rotating we are able to interpret it differently (in some moment after seeing the rotation when it stops, or perhaps only during certain moment during rotation).

I also noted that in some online examples it was relevant that we perceive the object starting from scanning bottom to top, so I noticed that my sketch is more interesting when the central object is shifted towards the bottom of the screen.
